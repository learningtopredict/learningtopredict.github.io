<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white; /* White text */
  padding: 5px 12px; /* Some padding */
  cursor: pointer; /* Pointer/hand icon */
  float: center; /* Float the buttons side by side */
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <meta name="theme-color" content="#ffffff" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150458464-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-150458464-1');
  </script>

  <!-- SEO -->
  <meta property="og:title" content="Learning to Predict Without Looking Ahead" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="World Models Without Forward Prediction" />
  <meta property="og:image" content="https://learningtopredict.github.io/assets/img/sns_card_rect.png" />
  <meta property="og:url" content="https://learningtopredict.github.io/" />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Learning to Predict Without Looking Ahead" />
  <meta name="twitter:description" content="World Models Without Forward Prediction" />
  <meta property="og:site_name" content="Learning to Predict Without Looking Ahead: World Models Without Forward Prediction" />
  <meta name="twitter:image" content="https://learningtopredict.github.io/assets/img/sns_card_square.png" />


</head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "Learning to Predict Without Looking Ahead: World Models Without Forward Prediction"
  description: ""
</script>
<body>

<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/learncartpole5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
</div>

<dt-article id="dtbody">

<div style="text-align: center;">
<figcaption style="text-align: left;color:#FF6C00;"><br/>Cartpole Swingup with Observational Dropout</figcaption>
<figcaption style="text-align: left;">
Our agents are only given infrequent observations of the real environment. As a side effect for optimizing performance in this setting, a “world model” emerges. We show the true dynamics in color, with full saturation denoting frames the policy can see. The black and white outline shows the state of the emergent world model. These world model exhibits similar, but not identical dynamics to forward predictive models but only model “important” aspects of the environment.<br/>
</figcaption>
</div>

<dt-byline class="l-page transparent"></dt-byline>

<h2>Learning to Predict Without Looking Ahead:<br/>World Models Without Forward Prediction</h2>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=t5Xsx0IAAAAJ">C. Daniel Freeman</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
    <div class="author">
        <a class="name" href="https://lukemetz.github.io/">Luke Metz</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
    <div class="author">
        <a class="name" href="http://otoro.net/ml/">David Ha</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
  </div>
  <div class="date">
    <div class="month">October 29</div>
    <div class="year">2019</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/1910.13038" target="_blank">PDF</a></div>
  </div>
  <div class="date">
    <div class="month">NeurIPS 2019</div>
    <div class="year" style="color: #FF6C00;"><a href="slides/neurips2019_poster.pdf" target="_blank">Poster</a></div>
  </div>
</div>
</dt-byline>
<h2>Abstract</h2>
<p>Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware--e.g., a brain--arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent.  That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call <em>observational dropout</em>, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into <em>learning</em> a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment.</p>
<hr>
<h2>Introduction</h2>
<p>Much of the motivation of model-based reinforcement learning (RL) derives from the potential utility of learned models for downstream tasks, like prediction <dt-cite key="doll2012ubiquity,finn2016unsupervised"></dt-cite>, planning <dt-cite key="allen1983planning,thrun1991planning,oh2015action,lenz2015deepmpc,nagabandi2018neural,nagabandi2018learning"></dt-cite>, and counterfactual reasoning <dt-cite key="buesing2018woulda,kaiser2019model"></dt-cite>. Whether such models are learned from data, or created from domain knowledge, there's an implicit assumption that an agent's <em>world model</em> <dt-cite key="werbos1987,schmidhuber1990making,ha2018world"></dt-cite> is a forward model for predicting future states. While a <em>perfect</em> forward model will undoubtedly deliver great utility, they are difficult to create, thus much of the research has been focused on either dealing with uncertainties of forward models <dt-cite key="deisenroth2011pilco,gal2016improving,ha2018world"></dt-cite>, or improving their prediction accuracy <dt-cite key="hafner2018learning,kaiser2019model"></dt-cite>. While progress has been made with current approaches, it is not clear that models trained explicitly to perform forward prediction is the only possible or even desirable solution.</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/learncartpole5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/paper_figure_1.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Our agent is given only infrequent observations of its environment (e.g., frames 1, 8), and must learn a world model to fill in the observation gaps. The colorless cart-pole represents the predicted observations seen by the policy. Under such constraints, we show that forward predictive-like world models can emerge so that the policy can still perform well on a cart-pole swing up environment.<br/>
</figcaption>
</div>
<p>We hypothesize that explicit forward prediction is not required to learn useful models of the world, and that prediction may arise as an emergent property if it is useful for an agent to perform its task. To encourage prediction to emerge, we introduce a constraint to our agent: at each timestep, the agent is only allowed to observe its environment with some probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>. To cope with this constraint, we give our agent an internal model that takes as input both the previous observation and action, and it generates a new observation as an output. Crucially, the input observation to the model will be the ground truth only with probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>, while the input observation will be its previously generated one with probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">1-p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit">p</span></span></span></span>. The agent's policy will act on this internal observation without knowing whether it is real, or generated by its internal model. In this work, we investigate to what extent world models trained with policy gradients behave like forward predictive models, by restricting the agent's ability to observe its environment.</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/learncarracing.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">
<b>Training an agent to drive a car blindfolded, but occasionally letting it see.</b>
<br/>The above animation illustrates the world model’s prediction on the right versus the ground truth pixel observation on the left. Frames that have a red border frames indicate actual observations from the environment the agent is allowed to see. The policy is acting on the observations (real or generated) on the right.<br/>
</figcaption>
</div>
<p>By jointly learning both the policy and model to perform well on the given task, we can directly optimize the model without ever explicitly optimizing for forward prediction. This allows the model to focus on generating any “predictions” that are useful for the policy to perform well on the task, even if they are not realistic. The models that emerge under our constraints capture the essence of what the agent needs to see from the world. We conduct various experiments to show, under certain conditions, that the models learn to behave like imperfect forward predictors. We demonstrate that these models can be used to generate environments that do not follow the rules that govern the actual environment, but nonetheless can be used to teach the agent important skills needed in the actual environment. We also examine the role of inductive biases in the world model, and show that the architecture of the model plays a role in not only in performance, but also interpretability.</p>
<hr>
<h2>Motivation:<br/>When a random world model is good enough</h2>
<p>A common goal when learning a world model is to learn a perfect forward predictor.  In this section, we provide intuitions for why this is not always necessary, and demonstrate how learning on random “world models” can lead to performant policies when transferred to the real world. For simplicity, we consider
the classical control task of balance cart-pole <dt-cite key="barto1983neuronlike"></dt-cite>.
While there are many ways of constructing world models for cart-pole, an optimal forward predictive model will have to generate trajectories of solutions to the simple linear differential equation describing the pole's dynamics near the unstable equilibrium point <dt-fn>In general, the full dynamics describing cart-pole is non-linear. However, in the limit of a heavy cart and small perturbations about the vertical at low speeds, it reduces to a linear system. See Appendix for details.</dt-fn>.  One particular coefficient matrix fully describes these dynamics, thus, for this example, we identify this coefficient matrix as the free parameters of the world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>.</p>
<p>While this unique <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> perfectly describe the dynamics of the pole, if our objective is only to stabilize the system--<em>not</em> achieve perfect forward prediction--it stands to reason that we may not necessarily need to know these exact dynamics.  In fact, if one solves for the linear feedback parameters that stabilize a cart-pole system with coefficient matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>M</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> (not necessarily equal to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>), for a wide variety of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>M</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, those same linear feedback parameters will also stabilize the “true” dynamics <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>.  Thus one successful, albeit silly strategy for solving balance cart-pole is choosing a random <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>M</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, finding linear feedback parameters that stabilize this <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>M</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, and then deploying those same feedback controls to the “real” model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>.  We provide the details of this procedure in the Appendix.</p>
<p>Note that the “world model” learned in this way is almost arbitrarily wrong.  It does not produce useful forward predictions, nor does it accurately estimate any of the parameters of the “real” world like the length of the pole, or the mass of the cart.  Nonetheless, it can be used to produce a successful stabilizing policy.  In sum, this toy problem exhibits three interesting qualities:</p>
<p><strong>1.</strong>  That a world model can be learned that produces a valid policy without needing a forward predictive loss.</p>
<p><strong>2.</strong>  That a world model need not itself be forward predictive (at all) to facilitate finding a valid policy.</p>
<p><strong>3.</strong>  That the inductive bias intrinsic to one's world model almost entirely controls the ease of optimization of the final policy.</p>
<p>Unfortunately, most real world environments are not this simple and will not lead to performant policies without ever observing the real world. Nonetheless, the underlying lesson--that a world model can be quite wrong, so long as it is wrong the in the “right” way--will be a recurring theme throughout.</p>
<hr>
<h2>Emergent world models by learning to fill in gaps</h2>
<p>In the previous section, we outlined a strategy for finding policies without even “seeing” the real world.  In this section, we relax this constraint and allow the agent to periodically switch between real observations and simulated observations generated by a world model.  We call this method <em>observational dropout</em>, inspired by <dt-cite key="srivastava2014dropout">dropout</dt-cite>.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/schematic.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
We introduce a setup called <i>observational dropout</i> to coerce the agent into learning what might be a predictive world model. When the environment is augmented with observational dropout, our controller is only given infrequent (i.e. 5% chance) observations of the real environment. As a side effect for optimizing performance in this setting, a “world model” emerges. In this work, we investigate to what extent world models trained with policy gradients behave like forward predictive models, by restricting the agent’s ability to see its environment.<br/>
</figcaption>
</div>
<p>Mechanistically, this amounts to a map between a single markov decision process (MDP) into a different MDP with an augmented state space.
Instead of only optimizing the agent in the real environment, with some probability, at every frame, the agent uses its internal world model to produce an observation of the world conditioned on its previous observation.
When samples from the real world are used, the state of the world model is reset to the real state--effectively resynchronizing the agent's model to the real world.</p>
<p>To show this, consider an MDP with states <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>∈</mo><mrow><mi mathvariant="script">S</mi></mrow></mrow><annotation encoding="application/x-tex">s \in \mathcal{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mrel">∈</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.075em;">S</span></span></span></span></span>, transition distribution <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>∼</mo><mi>P</mi><mrow><mo fence="true">(</mo><msup><mi>s</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">s^{t+1} \sim P\left(s^{t}, a^{t}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="minner textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;">)</span></span></span></span></span>, and reward distribution <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><msup><mi>s</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">R(s^{t}, a, s^{t+1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> we can create a new partially observed MDP with 2 states, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>=</mo><mo>(</mo><msub><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow></msub><mo separator="true">,</mo><msub><mi>s</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">s&#x27; = (s_{orig}, s_{model})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.038em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∈</mo></mrow><annotation encoding="application/x-tex">\in</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.5391em;"></span><span class="strut bottom" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="base textstyle uncramped"><span class="mrel">∈</span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mrow><mi mathvariant="script">S</mi></mrow><mo separator="true">,</mo><mrow><mi mathvariant="script">S</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{S}, \mathcal{S})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.075em;">S</span></span><span class="mpunct">,</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.075em;">S</span></span><span class="mclose">)</span></span></span></span>, consisting of both the original states, and the internal state produced by the world model. The transition function then switches between the real, and world model states with some probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/paper_eq_1.png" style="display: block; margin: auto; width: 65%;"/>
</div>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>∼</mo><mtext><mi mathvariant="normal">U</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi></mtext><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">r \sim \text{Uniform}(0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mrel">∼</span><span class="text mord textstyle uncramped"><span class="mord mathrm">U</span><span class="mord mathrm">n</span><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">m</span></span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">s^{t+1}_{orig}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.267211em;vertical-align:-0.412972em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.276864em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the real environment transition, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>∼</mo><mi>P</mi><mo>(</mo><msubsup><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow><mrow><mi>t</mi></mrow></msubsup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">s^{t+1}_{orig} \sim P(s^{t}_{orig}, a^{t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.267211em;vertical-align:-0.412972em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.276864em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.258664em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">s^{t+1}_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.1555469999999999em;vertical-align:-0.3013079999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.3013079999999999em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the next world model transition, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>s</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>∼</mo><mi>M</mi><mo>(</mo><msubsup><mi>s</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mrow><mi>t</mi></mrow></msubsup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">;</mo><mi>ϕ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s^{t+1}_{model} \sim M(s^{t}_{model}, a^{t}; \phi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.1555469999999999em;vertical-align:-0.3013079999999999em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.3013079999999999em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.2831079999999999em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">d</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit">ϕ</span><span class="mclose">)</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span> is the peek probability.</p>
<p>The observation space of this new partially observed MDP is always the second entry of the state tuple, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>s</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup></mrow><annotation encoding="application/x-tex">s&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.
As before, we care about performing well on the real environment thus the reward function is the same as the original environment: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>(</mo><msup><mi>s</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">,</mo><msup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>)</mo><mo>=</mo><mi>R</mi><mo>(</mo><msubsup><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow><mrow><mi>t</mi></mrow></msubsup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">,</mo><msubsup><mi>s</mi><mrow><mi>o</mi><mi>r</mi><mi>i</mi><mi>g</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">R&#x27;(s^{t}, a^{t}, s^{t+1}) = R(s^{t}_{orig}, a^{t}, s^{t+1}_{orig})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.854239em;"></span><span class="strut bottom" style="height:1.267211em;vertical-align:-0.412972em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.258664em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:0.276864em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span style="top:-0.403131em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>. Our learning task consists of training an agent, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi><mo>(</mo><mi>s</mi><mo separator="true">;</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\pi(s; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, and the world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>(</mo><mi>s</mi><mo separator="true">,</mo><msup><mi>a</mi><mrow><mi>t</mi></mrow></msup><mo separator="true">;</mo><mi>ϕ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">M(s ,a^{t}; \phi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:1.043556em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathit">s</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit">ϕ</span><span class="mclose">)</span></span></span></span> to maximize reward in this augmented MDP. In our work, we parameterize our world model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, and our policy <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span>, as neural networks with parameters <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϕ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span> respectively. While it's possible to optimize this objective with any reinforcement learning method <dt-cite key="schulman2015trust,mnih2015human,mnih2016asynchronous,schulman2017proximal"></dt-cite>, we choose to use population based REINFORCE <dt-cite key="williams1992simple"></dt-cite> due to its simplicity and effectiveness at achieving high scores on various tasks <dt-cite key="salimans2017evolution,ha2017evolving,ha2018designrl"></dt-cite>.
By restricting the observations, we make optimization harder and thus expect worse performance on the underlying task.
We can use this optimization procedure, however, to drive learning of the world model much in the same way evolution drove our internal world models.</p>
<p>One might worry that a policy with sufficient capacity could extract useful data from a world model, even if that world model's features weren't easily interpretable.  In this limit, our procedure starts looking like a strange sort of recurrent network, where the world model “learns” to extract difficult-to-interpret features (like, e.g., the hidden state of an RNN) from the world state, and then the policy is powerful enough to learn to use these features to make decisions about how to act.  While this is indeed a possibility, in practice, we usually constrain the capacity of the policies we studied to be small enough that this did not occur.  For a counter-example, see the fully connected world model for the grid world tasks described later.</p>
<hr>
<h2>What policies can be learned from world models emerged from observation dropout?</h2>
<p>As the balance cart-pole task discussed earlier can be trivially solved with a wide range of parameters for a simple linear policy, we conduct experiments where we apply observational dropout on the more difficult swing up cart-pole--a task that cannot be solved with a linear policy, as it requires the agent to learn two distinct subtasks:</p>
<p><strong>1.</strong>  To add energy to the system when it needs to swing up the pole.</p>
<p><strong>2.</strong>  To remove energy to balance the pole once the pole is close to the unstable, upright equilibrium <dt-cite key="tedrake2009underactuated"></dt-cite>.</p>
<p>Our setup is closely based on the environment described in <dt-cite key="gal2016improving,deepPILCOgithub"></dt-cite>, where the ground truth dynamics of the environment is described as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mover accent="true"><mrow><mi>x</mi></mrow><mo>¨</mo></mover><mo separator="true">,</mo><mover accent="true"><mrow><mi>θ</mi></mrow><mo>¨</mo></mover><mo>]</mo><mo>=</mo><mi>F</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mover accent="true"><mrow><mi>x</mi></mrow><mo>˙</mo></mover><mo separator="true">,</mo><mover accent="true"><mrow><mi>θ</mi></mrow><mo>˙</mo></mover><mo>)</mo></mrow><annotation encoding="application/x-tex">[\ddot{x}, \ddot{\theta}] = F(x, \theta, \dot{x}, \dot{\theta})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9313em;"></span><span class="strut bottom" style="height:1.1813em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">[</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">x</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¨</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>¨</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">]</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">x</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>˙</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>˙</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">F</span></span></span></span> is a system of non-linear equations, and the agent is rewarded for getting <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> close to zero and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">cos(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> close to one.</p>
<p>We can visualize the cart-pole experiment after training our agent inside the cart-pole swing up environment augmented with observational dropout:</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/learncartpole5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/paper_figure_1.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Our agent is given only infrequent observations of its environment (e.g., frames 1, 8), and must learn a world model to fill in the observation gaps. The colorless cart-pole represents the predicted observations seen by the policy. Under such constraints, we show that forward predictive-like world models can emerge so that the policy can still perform well on a cart-pole swing up environment.<br/>
</figcaption>
</div>
<p>As a sanity check, we can confirm that the policy that is jointly learned with the world model learns a policy that also works when observational dropout is disabled in the environment:</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/controller5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">
The policy that is jointly learned with the world model, deployed in the original environment (without observational dropout) where the agent can see the actual observations at each timestep.<br/>
</figcaption>
</div>
<p>In the figure below, we report the performance of our agent trained in environments with various peek probabilities, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>. A result higher than <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 500 means that the agent is able to swing up and balance the cart-pole most of the time. Interestingly, the agent is still able to solve the task even when on looking at a tenth of the frames (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p=10\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.94444em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">%</span></span></span></span>), and even at a lower <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p=5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.94444em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mrel">=</span><span class="mord mathrm">5</span><span class="mord mathrm">%</span></span></span></span>, it solves the task half of the time.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/svg/cartpole_performance.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Performance of cart-pole swing up under various observational dropout probabilities, <i>p</i>.  Here, both the policy and world model are learned.<br/>
</figcaption>
</div>
<p>To understand the extent to which the policy, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span> relies on the learned world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, and to probe the dynamics learned world model, we trained a new policy entirely within learned world model and then deployed these policies back to the original environment. The results are shown in the figure below:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/svg/cartpole_dream.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Performance of deploying policies trained from scratch inside of the environment generated by the world model, in the actual environment. For each <i>p</i>, the experiment is run 10 times independently (orange). Performance is measured by averaging cumulative scores over 100 rollouts.  Model-based baseline performances learned via a forward-predictive loss are indicated in red, blue.  Note how world models learned when trained under approximately 3-5% observational dropout can be used to train performant policies.<br/>
</figcaption>
</div>
<p>Qualitatively, the agent learns to swing up the pole, and balance it for a short period of time when it achieves a mean reward above <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 300.
Below this threshold the agent typically swings the pole around continuously, or navigates off the screen.
We observe that at low peek probabilities, a higher percentage of learned world models can be used to train policies that behave correctly under the actual dynamics, despite failing to completely solve the task.
At higher peek probabilities, the learned dynamics model is not needed to solve the task thus is never learned.</p>
<p>We have compared our approach to baseline model-based approach where we explicitly train our model to predict the next observation on a dataset collected from training a model-free agent from scratch to solving the task. To our surprise, we find it interesting that our approach can produce models that outperform an explicitly learned model with the same architecture size (120 units) for cart-pole transfer task. This advantage goes away, however, if we scale up the forward predictive model width by 10x.</p>
<p>In the current setup, the world model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> is trained as part of the agent's policy, but we would also like to examine whether we can use this model to <em>generate</em> the environment it has trained on. To examine the kind of world our model has learned, we attempt to train a policy (from scratch) inside an open loop environment generated by this world model:</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/dream5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">
In the generated environment, the cart-pole stabilizes at an angle that is not perfectly perpendicular, due to its imperfect nature. The world model that generated this environment is jointly trained with an observational dropout probability of <i>p=5%</i>.<br/>
</figcaption>
</div>
<p>The figure above depicts a trajectory of a policy trained entirely within a learned world model deployed on the actual environment.  It is interesting to note that the dynamics in the world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, are not perfect--for instance, the optimal policy inside the world model can only swing up and balance the pole at an angle that is not perpendicular to the ground.
We notice in other world models, the optimal policy learns to swing up the pole and only balance it for a short period of time, even in the self-contained world model.
It should not surprise us then, that the most successful policies when deployed back to the actual environment can swing up and only balance the pole for a short while, before the pole falls down, as visualized in the following figure:</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/deploy5.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">
This policy is still able to swing up the cart-pole in the actual environment, although it remains balanced only for some time before falling down.<br/>
</figcaption>
</div>
<p>As noted earlier, the task of stabilizing the pole once it is near its target state (when <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>x</mi></mrow><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.66786em;"></span><span class="strut bottom" style="height:0.66786em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">x</span></span></span><span style="top:0em;margin-left:0.05556em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>˙</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>θ</mi></mrow><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9313em;"></span><span class="strut bottom" style="height:0.9313em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-0.26343999999999995em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>˙</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is near zero) is trivial, hence a policy, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span>, jointly trained with world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, will not require accurate predictions to keep the pole balanced.
For this subtask, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span> needs only to occasionally observe the actual world and realign its internal observation with reality.
Conversely, the subtask of swinging the pole upwards and then lowering the velocities is much more challenging, hence <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span> will rely on the world model to captures the essence of the dynamics for it to accomplish the subtask.
The world model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> only learns the <em>difficult</em> part of the real world, as that is all that is required of it to facilitate the policy performing well on the task.</p>
<hr>
<h2>Examining world models' inductive biases in a grid world</h2>
<p>To illustrate the generality of our method to more varied domains, and to further emphasize the role played by inductive bias in our models, we consider an additional problem: a classic search / avoidance task in a grid world.  In this problem, an agent navigates a grid environment with randomly placed apples and fires.  Apples provide reward, and fires provide negative reward.  The agent is allowed to move in the four cardinal directions, or to perform a no-op. For a detailed description of the grid world environment, please refer to the Appendix.</p>
<p>For simplicity, we considered only stateless policies and world models.  While this necessarily limits the expressive capacity of our world models, the optimal forward predictive model within this class of networks is straightforward to consider: movement of the agent essentially corresponds to a bit-shift map on the world model's observation vectors.  For example, for an optimal forward predictor, if an agent moves rightwards, every apple and fire within its receptive field should shift to the left.  The leftmost column of observations shifts out of sight, and is forgotten--as the model is stateless--and the rightmost column of observations should be populated according to some distribution which depends on the locations of apples and fires visible to the agent, as well as the particular scheme used to populate the world with apples and fires. The figure below illustrates the receptive field of the world model:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/MovementCartoonRobotBetter.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
A cartoon demonstrating the shift of the receptive field of the world model as it moves to the right. The greyed out column indicates the column of forgotten data, and the light blue column indicates the “new” information gleaned from moving to the right. An optimal predictor would learn the distribution function <i>p</i> and sample from it to populate this rightmost column, and would match the ground truth everywhere else. The rightmost heat map illustrates how predictions of a convolutional model correlate with the ground truth (more orange = more predictive) when moving to the right, averaged over 1000 randomized right-moving steps. Crucially, this heat map is most predictive for the cells the agent can actually see, and is less predictive for the cells right outside its field of view (the rightmost column) as expected.<br/>
</figcaption>
</div>
<p>This partial observability of the world immediately handicaps the ability of the world model to perform long imagined trajectories in comparison with the previous continuous, fully observed cart-pole tasks.  Nonetheless, there remains sufficient information in the world to train world models via observational dropout that are predictive.</p>
<p>For our numerical experiments we compared two different world model architectures: a fully connected model and a convolutional model (See Appendix for architecture details). Naively, these models are listed in increasing order of inductive bias, but decreasing order of overall capacity (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mn>0</mn><mn>6</mn><mn>5</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">10650</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">6</span><span class="mord mathrm">5</span><span class="mord mathrm">0</span></span></span></span> parameters for the fully connected model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mn>2</mn><mn>0</mn><mn>1</mn></mrow><annotation encoding="application/x-tex">1201</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">2</span><span class="mord mathrm">0</span><span class="mord mathrm">1</span></span></span></span> learnable parameters for the convolutional model)--i.e., the fully connected architecture has the highest capacity and the least bias, whereas the convolutional model has the most bias but the least capacity. As in the cart-pole tasks, we trained the agent's policy and world model jointly, where with some probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span> the agent sees the ground truth observation instead of predictions from its world model. The performance of these models on the task as a function of peek probability is provided in the figure below:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/grid_perf.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Performance, <i>R</i> of the two architectures, empirically averaged over hundred policies and a thousand rollouts as a function of peek probability, <i>p</i>.  The convolutional architecture reliably out performs the fully connected architecture. Error bars indicate standard error. Intuitively, a score near <i>0</i> amounts to random motion on the lattice—encountering apples as often as fires, and <i>2</i> approximately corresponds to encountering apples two to three times more often than fires.  A baseline that is trained on a version of the environment without any fires—i.e., a proxy baseline for an agent that can perfectly avoid fires—reliably achieves a score of <i>3</i>.<br/>
</figcaption>
</div>
<p>Curiously, even though the fully connected architecture has the highest overall capacity, and is capable of learning a transition map closer to the “optimal” forward predictive function for this task if taught to do so via supervised learning of a forward-predictive loss, it reliably performs worse than the convolutional architectures on the search and avoidance task. This is not entirely surprising: the convolutional architectures induce a considerably better prior over the space of world models than the fully connected architecture via their translational invariance. It is comparatively much easier for the convolutional architectures to randomly discover the right sort of transition maps.</p>
<p>Because the world model is not being explicitly optimized to achieve forward prediction, it doesn't often learn a predictive function for every direction.  We selected a typical convolutional world model and plot its empirically averaged correlation with the ground truth next-frames in the following figure:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/near_conv_correlations_one_row.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Empirically averaged correlation matrices between a world model's output and the ground truth. Averages were calculated using 1,000 random transitions for each direction of a typical convolutional <i>p=75%</i> world model.  Higher correlation (yellow-white) translates to a world model that is closer to a next frame predictor. Note that a predictive map is not learned for every direction. The row and column, respectively of dark pixels for ⬇ and ➡ correspond exactly to the newly-seen pixels for those directions<br/>
</figcaption>
</div>
<p>Here, the world model clearly only learns reliable transition maps for moving down and to the right, which is sufficient.
Qualitatively, we found that the convolutional world models learned with peek-probability close to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>5</mn><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p=50\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:0.94444em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mrel">=</span><span class="mord mathrm">5</span><span class="mord mathrm">0</span><span class="mord mathrm">%</span></span></span></span> were “best” in that they were more likely to result in accurate transition maps--similar to the cart-pole results indicated earlier.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/near_conv_correlations_2.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Correlation matrices for several sampled convolutional architectures. The dark pixel immediately adjacent to the agent in many of the correlation plots is a result of the agent failing to predict its own consumption of an apple, because the model used was translationally invariant.<br/>
</figcaption>
</div>
<p>Fully connected world models, on the other hand, reliably learned completely uninterpretable transition maps. That policies could <em>almost</em> achieve the same performance with fully connected world models as with convolutional world model is reminiscent of a recurrent architecture that uses the (generally not-easily-interpretable) hidden state as a feature.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/fc_correlations_2.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Correlation matrices for several sampled fully connected architectures. Note the lack of interpretability of the learned models, even though the policies learned jointly with these world models were fairly performant.<br/>
</figcaption>
</div>
<hr>
<h2>Car Racing: Keep your eyes <em>off</em> the road</h2>
<p>In more challenging environments, observations are often expressed as high dimensional pixel images rather than state vectors.
In this experiment, we apply observation dropout to learn a world model of a car racing game from pixel observations. We would like to know to what extent the world model can facilitate the policy at driving if the agent is only allowed to see the road only only a fraction of the time. We are also interested in the representations the model learns to facilitate driving, and in measuring the usefulness of its internal representation for this task.</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/learncarracing.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">
<br/>The above animation illustrates the world model’s prediction on the right versus the ground truth pixel observation on the left. Frames that have a red border frames indicate actual observations from the environment the agent is allowed to see. The policy is acting on the observations (real or generated) on the right.<br/>
</figcaption>
</div>
<p>In Car Racing <dt-cite key="carracing_v0"></dt-cite>, the agent's goal is to drive around the tracks, which are randomly generated for each trial, and drive over as many tiles as possibles in the shortest time. At each timestep, the environment provides the agent with a high dimensional pixel image observation, and the agent outputs 3 continuous action parameters that control the car's steering, acceleration, and brakes.</p>
<p>To reduce the dimensionality of the pixel observations, we follow the procedure in <dt-cite key="ha2018world"></dt-cite> and train a Variational Autoencoder (VAE) <dt-cite key="vae,vae_dm"></dt-cite> using on rollouts collected from a random policy, to compress a pixel observation into a small dimensional latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>. Our agent will use <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span> instead as its observation. Our policy, a feed forward network, will act on actual observations with probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>, otherwise on observations produced by the world model. Examples of pixel observations, and reconstructions from their compressed representations are shown in the first 2 rows of the following figure:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/png;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/png/carracing_demonstration.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Two examples of action-conditioned predictions from a world model trained at <i>p=10%</i> (bottom rows). Red boxes indicate actual observations from the environment the agent is allowed to see. While the agent is devoid of sight, the world model predicts <b>(1)</b> small movements of the car relative to the track and <b>(2)</b> upcoming turns. Without access to actual observations for many timesteps, it incorrectly predicts a turn in <b>(3)</b> until an actual observation realigns the world model with reality.<br/>
</figcaption>
</div>
<p>Our world model, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, a small feed forward network with a hidden layer, outputs the change of the mean latent vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span>, conditioned on the previous observation (actual or predicted) and action taken (i.e <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>z</mi><mo>=</mo><mi>M</mi><mo>(</mo><mi>z</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\Delta z = M(z, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Δ</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span>). We can use the VAE's decoder to visualize the latent vectors produced by <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span>, and compare them with the actual observations that the agent is not able to see (See figure above). We observe that our world model, while not explicitly trained to predict future frames, are still able to make meaningful action-conditioned predictions. The model also learns to predict local changes in the car's position relative to the road given the action taken, and also attempts to predict upcoming curves.</p>
<p>Our policy <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span> is jointly trained with world model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">M</span></span></span></span> in the car racing environment augmented with a peek probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span>. The agent's performance is reported in the figure below:</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/svg/carracing_performance.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Mean performance of Car Racing under various <i>p</i> over 100 trials.<br/>
</figcaption>
</div>
<p>Qualitatively, a score above <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 800 means that the agent can navigate around the track, making the occasional driving error. We see that the agent is still able to perform the task when 70% of the actual observation frames are dropped out, and the world model is relied upon to fill in the observation gaps for the policy.</p>
<p>If the world model produces useful predictions for the policy, then its hidden representation used to produce the predictions should also be useful features to facilitate the task at hand.
We can test whether the hidden units of the world model are directly useful for the task, by first freezing the weights of the world model, and then training from scratch a <em>linear</em> policy using only the outputs of the intermediate hidden layer of the world model as the only inputs.
This feature vector extracted the hidden layer will be mapped directly to the 3 outputs controlling the car, and we can measure the performance of a linear policy using features of world models trained at various peek probabilities.</p>
<div style="text-align: center;">
<img class="b-lazy" src=data:image/svg;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/svg/carracing_dream.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">
Mean performance achieved by training a linear policy using only the outputs of the hidden layer of a world model learned at peek probability <i>p</i>.
We run 5 independent seeds for each <i>p</i> (orange).
Model-based baseline performances learned via a forward-predictive loss are indicated in red, blue. We note that in this constrained linear policy setup, our best solution out of a population of trials achieves a performance slightly below reported state-of-the-art results (i.e. <dt-cite key="ha2018world,risi2019"></dt-cite>). As in the swingup cartpole experiments, the best world models for training policies occur at a characteristic peek probability that roughly coincides with the peek probability at which performance begins to degrade for jointly trained models (i.e., the bend in the previous figure occurs near the peak of the this figure).<br/>
</figcaption>
</div>
<p>The results reported in the above figure show that world models trained at lower peek probabilities have a higher chance of learning features that are useful enough for a linear controller to achieve an average score of 800. The average performance of the linear controller peaks when using models trained with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span></span></span></span> around 40%. This suggests that a world model will learn more useful representation when the policy needs to rely more on its predictions as the agent's ability to observe the environment decreases. However, a peek probability too close to zero will hinder the agent's ability to perform its task, especially in non-deterministic environments such as this one, and thus also affect the usefulness of its world model for the real world, as the agent is almost completely disconnected from reality.</p>
<hr>
<h2>Related Work</h2>
<p>One promising reason to learn models of the world is to accelerate learning of policies by training these models.
These works obtain experience from the real environment, and fit a model directly to this data.
Some of the earliest work leverage simple model parameterizations--e.g. learnable parameters for system identification <dt-cite key="pillonetto2014kernel"></dt-cite>.
Recently, there has been large interest in using more flexible parameterizations in the form of function approximators.
The earliest work we are aware of that uses feed forward neural networks as predictive models for tasks is <dt-cite key="werbos1987"></dt-cite>.
To model time dependence, recurrent neural network were introduced in <dt-cite key="schmidhuber1990making"></dt-cite>. Recently, as our modeling abilities increased, there has been renewed interest in directly modeling pixels <dt-cite key="srivastava2015unsupervised,patraucean2015spatio,kalchbrenner2017video,hafner2018learning"></dt-cite>. <dt-cite key="mathieu2015deep"></dt-cite> modify the loss function used to generate more realistic predictions. <dt-cite key="denton2018stochastic"></dt-cite> propose a stochastic model which learns to predict the next frame in a sequence, whereas <dt-cite key="finn2016unsupervised"></dt-cite> employ a different parameterization involving predicting pixel movement as opposed to directly predicting pixels.
<dt-cite key="kumar2019videoflow"></dt-cite> employ flow based tractable density models to learn models, and <dt-cite key="ha2018world"></dt-cite> leverages a VAE-RNN architecture to learn an embedding of pixel data across time.
<dt-cite key="hafner2018learning"></dt-cite> propose to learn a latent space, and learn forward dynamics in this latent space.
Other methods utilize probabilistic dynamics models which allow for better planning in the face of uncertainty <dt-cite key="deisenroth2011pilco,gal2016improving"></dt-cite>.
Presaging much of this work is <dt-cite key="silver2017predictron"></dt-cite>, which learns a model that can predict environment state over multiple timescales via imagined rollouts.</p>
<p>As both predictive modeling and control improves there has been a large number of successes leveraging learned predictive models in Atari <dt-cite key="buesing2018learning,kaiser2019model"></dt-cite> and robotics <dt-cite key="ebert2018visual"></dt-cite>.
Unlike our work, all of these methods leverage transitions to learn an explicit dynamics model.
Despite advances in forward predictive modeling, the application of such models is limited to relatively simple domains where models perform well.</p>
<p>Errors in the world model compound, and cause issues when used for control <dt-cite key="talvitie2014model,asadi2018lipschitz"></dt-cite>. <dt-cite key="amos2018differentiable"></dt-cite>, similar to our work, directly optimizes the dynamics model against loss by differentiating through a planning procedure, and <dt-cite key="schmidhuber2015learning"></dt-cite> proposes a similar idea of improving the internal model using an RNN, although the RNN world model is initially trained to perform forward prediction.
In this work we structure our learning problem so a model of the world will emerge as a result of solving a given task.
This notion of emergent behavior has been explored in a number of different areas and broadly is called “representation learning” <dt-cite key="bengio2013representation"></dt-cite>.
Early work on autoencoders leverage reconstruction based losses to learn meaningful features <dt-cite key="hinton2006reducing,le2011building"></dt-cite>.
Follow up work focuses on learning “disentangled” representations by enforcing more structure in the learning procedure <dt-cite key="higgins2016early,higgins2018towards"></dt-cite>.
Self supervised approaches construct other learning problems, e.g. solving a jigsaw puzzle <dt-cite key="noroozi2016unsupervised"></dt-cite>, or leveraging temporal structure <dt-cite key="sermanet2018time,oord2018representation"></dt-cite>. Alternative setups, closer to our own specify a specific learning problem and observe that by solving these problems lead to interesting learned behavior (e.g. grid cells) <dt-cite key="cueva2018emergence,banino2018vector"></dt-cite>. In the context of learning models, <dt-cite key="watter2015embed"></dt-cite> construct a locally linear latent space where planning can then be performed.</p>
<p>The force driving model improvement in our work consists of black box optimization. In an effort to emulate nature, evolutionary algorithms where proposed <dt-cite key="holland1975adaptation,goldberg1988genetic,hansen2003reducing,wierstra2008natural,such2017deep"></dt-cite>. These algorithms are robust and will adapt to constraints such as ours while still solving the given task <dt-cite key="bongard2006resilient,lehman2018surprising"></dt-cite>. Recently, reinforcement learning has emerged as a promising framework to tackle optimization leveraging the sequential nature of the world for increased efficiency <dt-cite key="sutton1998introduction,schulman2015trust,mnih2015human,mnih2016asynchronous,schulman2017proximal"></dt-cite>. The exact type of the optimization is of less importance to us in this work and thus we choose to use a simple population-based optimization algorithm <dt-cite key="williams1992simple"></dt-cite> with connections to evolution strategies <dt-cite key="rechenberg1973evolutionsstrategie,schwefel1977numerische,salimans2017evolution"></dt-cite>.</p>
<p>The boundary between what is considered <em>model-free</em> and <em>model-based</em> reinforcement learning is blurred when one can considers both the model network and controller network together as one giant policy that can be trained end-to-end with model-free methods. <dt-cite key="risi2019"></dt-cite> demonstrates this by training both world model and policy via evolution. <dt-cite key="marques2007sensorless"></dt-cite> explore modifying sensor information similarly to our observational dropout. Instead of performance, however, this work focus on understanding what these models learn and show there usefulness--e.g. training a policy inside the learned models.</p>
<hr>
<h2>Discussion</h2>
<p>In this work, we explore world models that emerge when training with <em>observational dropout</em> for several reinforcement learning tasks.  In particular, we've demonstrated how effective world models can emerge from the optimization of total reward. Even on these simple environments, the emerged world models do not perfectly model the world, but they facilitate policy learning well enough to solve the studied tasks.</p>
<p>The deficiencies of the world models learned in this way have a consistency: the cart-pole world models learned to swing up the pole, but did not have a perfect notion of equilibrium--the grid world models could perform reliable bit-shift maps, but only in certain directions--the car racing world model tended to ignore the forward motion of the car, unless a turn was visible to the agent (or imagined).  Crucially, none of these deficiencies were catastrophic enough to cripple the agent's performance.  In fact, these deficiencies were, in some cases, irrelevant to the performance of the policy.  We speculate that the complexity of world models could be greatly reduced if they could fully leverage this idea:  that a complete model of the world is actually unnecessary for most tasks--that by identifying the <em>important</em> part of the world, policies could be trained significantly more quickly, or more sample efficiently.</p>
<p>We hope this work stimulates further exploration of both model based and model free reinforcement learning, particularly in areas where learning a perfect world model is intractable.</p>
<p><em>If you would like to discuss any issues or give feedback, please visit the <a href="https://github.com/learningtopredict/learningtopredict.github.io/issues">GitHub</a> repository of this page for more information.</em></p>
</dt-article>
<dt-appendix>
<h2>Acknowledgements</h2>
<p>We would like to thank our three reviewers for their helpful comments.  Additionally, we would like to thank Alex Alemi, Tom Brown, Douglas Eck, Jaehoon Lee, Błażej Osiński, Ben Poole, Jascha Sohl-Dickstein, Mark Woodward, Andrea Benucci, Julian Togelius, Sebastian Risi, Hugo Ponte, and Brian Cheung for helpful comments, discussions, and advice on early versions of this work.</p>
<p>The experiments in this work were performed on 96-core CPU Linux virtual machines provided by <a href="https://cloud.google.com/">Google Cloud Platform</a>.</p>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>.</p>
<p>Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the discussion <a href="https://github.com/learningtopredict/learningtopredict.github.io/issues">forum</a> for this article.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">C. Daniel Freeman and Luke Metz and David Ha,<br/>
"Learning to Predict Without Looking Ahead: World Models Without Forward Prediction", 2019.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{learningtopredict2019,
  author = {Freeman, C Daniel and Luke Metz and David Ha},
  title  = {Learning to Predict Without Looking Ahead: World Models Without Forward Prediction},
  eprint = {arXiv:1910.13038},
  url    = {https://learningtopredict.github.io},
  note   = "\url{https://learningtopredict.github.io}",
  year   = {2019}
}</pre>
<h2>Open Source Code</h2>
<p>Please see our <a href="https://github.com/google/brain-tokyo-workshop/">repo</a> for details about the code release. (Not yet released, will be there soon!)</p>
<h2>Reuse</h2>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="https://github.com/learningtopredict/learningtopredict.github.io">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.</p>
<h2>Appendix</h2>
<p>For further discussion about theoretical parts of the paper (relating to cart-pole), and also the implementation details of the experiments, please refer to the Appendix section in the <a href="https://arxiv.org/abs/1910.13038">pdf</a> version.</p>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@inproceedings{silver2017predictron,
  title={The predictron: End-to-end learning and planning},
  author={Silver, David and van Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and others},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3191--3199},
  year={2017},
  organization={JMLR.org},
  url={https://arxiv.org/abs/1612.08810},
}

@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}


@article{hafner2018learning,
  title={Learning Latent Dynamics for Planning from Pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  journal={arXiv preprint arXiv:1811.04551},
  year={2018},
  url={https://planetrl.github.io},
}

@article{kaiser2019model,
  title={Model-Based Reinforcement Learning for Atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  url={https://arxiv.org/abs/1903.00374},
  year={2019}
}

@article{du2019implicit,
  title={Implicit Generation and Generalization in Energy-Based Models},
  author={Du, Yilun and Mordatch, Igor},
  journal={arXiv preprint arXiv:1903.08689},
  year={2019},
  url={https://arxiv.org/abs/1903.08689},
}

@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011},
  url={http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
}

@inproceedings{gal2016improving,
  title={Improving PILCO with Bayesian neural network dynamics models},
  author={Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle={Data-Efficient Machine Learning workshop, ICML},
  volume={4},
  year={2016},
  url={http://mlg.eng.cam.ac.uk/yarin/PDFs/DeepPILCO.pdf},
}

@misc{deepPILCOgithub,
      author = {Zuo, Xingdong},
      title = {PyTorch implementation of Improving PILCO with Bayesian neural network dynamics models},
      year = {2018},
      publisher = {GitHub},
      journal = {GitHub repository},
      url = {https://github.com/zuoxingdong/DeepPILCO},
    }

@article{tedrake2009underactuated,
  title={Underactuated Robotics: Learning, Planning, and Control for Efficient and Agile Machines: Course Notes for MIT 6.832},
  author={Tedrake, Russ},
  journal={Working draft edition},
  volume={3},
  year={2009},
  publisher={Citeseer},
  url={http://underactuated.mit.edu/underactuated.html},
}

@inproceedings{watter2015embed,
  title={Embed to control: A locally linear latent dynamics model for control from raw images},
  author={Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
  booktitle={Advances in neural information processing systems},
  pages={2746--2754},
  year={2015},
  url={https://arxiv.org/abs/1506.07365},
}

@inproceedings{werbos1987,
  title={Learning how the world works: Specifications for predictive networks in robots and brains},
  author={Werbos, Paul J},
  booktitle={Proceedings of IEEE International Conference on Systems, Man and Cybernetics, NY},
  year={1987}
}

@article{schmidhuber1990making,
  title={Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments},
  author={Schmidhuber, Juergen},
  year={1990},
  journal={Technical Report},
  url={http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf},
}

@article{banino2018vector,
  title={Vector-based navigation using grid-like representations in artificial agents},
  author={Banino, Andrea and Barry, Caswell and Uria, Benigno and Blundell, Charles and Lillicrap, Timothy and Mirowski, Piotr and Pritzel, Alexander and Chadwick, Martin J and Degris, Thomas and Modayil, Joseph and others},
  journal={Nature},
  volume={557},
  number={7705},
  pages={429},
  year={2018},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s41586-018-0102-6.epdf},
}

@article{cueva2018emergence,
  title={Emergence of grid-like representations by training recurrent neural networks to perform spatial localization},
  author={Cueva, Christopher J and Wei, Xue-Xin},
  journal={arXiv preprint arXiv:1803.07770},
  year={2018},
  url={https://arxiv.org/abs/1803.07770},
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, N and Hinton, G and Krizhevsky, A and Sutskever, I and Salakhutdinov, R},
  journal={JMLR},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR.org},
  url={http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf},
}

@incollection{ha2018world,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, Juergen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  pages = {2451--2463},
  year = {2018},
  url = {https://worldmodels.github.io/},
}

@inproceedings{risi2019,
 author = {Risi, Sebastian and Stanley, Kenneth O.},
 title = {Deep Neuroevolution of Recurrent and Discrete World Models},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
 series = {GECCO '19},
 year = {2019},
 isbn = {978-1-4503-6111-8},
 location = {Prague, Czech Republic},
 pages = {456--462},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3321707.3321817},
 doi = {10.1145/3321707.3321817},
 acmid = {3321817},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{marques2007sensorless,
  title={Sensorless but not senseless: Prediction in evolutionary car racing},
  author={Marques, Hugo and Togelius, Julian and Kogutowska, Magdalena and Holland, Owen and Lucas, Simon M},
  booktitle={2007 IEEE Symposium on Artificial Life},
  pages={370--377},
  year={2007},
  organization={IEEE},
  url={http://julian.togelius.com/Marques2007Sensorless.pdf},
}

@article{gaier2019weight,
  title={Weight Agnostic Neural Networks},
  author={Gaier, Adam and Ha, David},
  journal={arXiv preprint arXiv:1906.04358},
  year={2019},
  url={https://weightagnostic.github.io},
}

@article{denton2018stochastic,
  title={Stochastic video generation with a learned prior},
  author={Denton, Emily and Fergus, Rob},
  journal={arXiv preprint arXiv:1802.07687},
  year={2018},
  url={https://arxiv.org/abs/1802.07687},
}

@inproceedings{finn2016unsupervised,
  title={Unsupervised learning for physical interaction through video prediction},
  author={Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  booktitle={Advances in neural information processing systems},
  pages={64--72},
  year={2016},
  url={https://arxiv.org/abs/1605.07157},
}

@article{kumar2019videoflow,
  title={VideoFlow: A Flow-Based Generative Model for Video},
  author={Kumar, M and Babaeizadeh, M and Erhan, D and Finn, C and Levine, S and Dinh, L and Kingma, D},
  journal={arXiv preprint arXiv:1903.01434},
  year={2019},
  url={https://arxiv.org/abs/1903.01434},
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  url={https://arxiv.org/abs/1602.01783},
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer},
  url={http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf},
}
% proof that williams1992simple can perform really well:
@article{salimans2017evolution,
  title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
 author = {Salimans, T. and Ho, J. and Chen, X. and Sidor, S. and Sutskever, I.},
journal={Preprint arXiv:1703.03864},
  year={2017},
  url={https://arxiv.org/abs/1703.03864},
}
@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017},
  url={https://arxiv.org/abs/1712.06567},
}
@article{ha2017evolving,
  title={Evolving Stable Strategies},
  author = {Ha, D.},
  year={2017},
  url="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/",
}
@article{ha2018designrl,
  author = {David Ha},
  title  = {Reinforcement Learning for Improving Agent Design},
  journal = {arXiv:1810.03779},
  url    = {https://designrl.github.io},
  year   = {2018}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018},
  url={https://arxiv.org/abs/1807.03748},
}

@article{pillonetto2014kernel,
  title={Kernel methods in system identification, machine learning and function estimation: A survey},
  author={Pillonetto, Gianluigi and Dinuzzo, Francesco and Chen, Tianshi and De Nicolao, Giuseppe and Ljung, Lennart},
  journal={Automatica},
  volume={50},
  number={3},
  pages={657--682},
  year={2014},
  publisher={Elsevier},
  url={http://www.diva-portal.org/smash/get/diva2:716642/FULLTEXT01.pdf},
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science},
  url={https://bit.ly/2Bvc6KM},
}

@article{higgins2016early,
  title={Early visual concept learning with unsupervised deep learning},
  author={Higgins, Irina and Matthey, Loic and Glorot, Xavier and Pal, Arka and Uria, Benigno and Blundell, Charles and Mohamed, Shakir and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1606.05579},
  year={2016},
  url={https://arxiv.org/abs/1606.05579},
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/abs/1707.06347},
}

@inproceedings{allen1983planning,
  title={Planning using a temporal world model},
  author={Allen, James F and Koomen, Johannes A},
  booktitle={Proceedings of the Eighth international joint conference on Artificial intelligence-Volume 2},
  pages={741--747},
  year={1983},
  organization={Morgan Kaufmann Publishers Inc.},
  url={https://www.ijcai.org/Proceedings/83-2/Papers/036.pdf},
}

@inproceedings{thrun1991planning,
  title={Planning with an adaptive world model},
  author={Thrun, Sebastian and Moller, Knut and Linden, Alexander},
  booktitle={Advances in neural information processing systems},
  pages={450--456},
  year={1991},
  url={https://papers.nips.cc/paper/365-planning-with-an-adaptive-world-model.pdf},
}

@article{doll2012ubiquity,
  title={The ubiquity of model-based reinforcement learning},
  author={Doll, Bradley B and Simon, Dylan A and Daw, Nathaniel D},
  journal={Current opinion in neurobiology},
  volume={22},
  number={6},
  pages={1075--1081},
  year={2012},
  publisher={Elsevier},
  url={http://www.princeton.edu/~ndaw/dsd12.pdf},
}

@article{buesing2018woulda,
  title={Woulda, coulda, shoulda: Counterfactually-guided policy search},
  author={Buesing, Lars and Weber, Theophane and Zwols, Yori and Racaniere, Sebastien and Guez, Arthur and Lespiau, Jean-Baptiste and Heess, Nicolas},
  journal={arXiv preprint arXiv:1811.06272},
  year={2018},
  url={https://arxiv.org/abs/1811.06272},
}

@article{grupencourse,
title={CMPSCI Embedded Systems 503},
author={Roderic A. Grupen},
url={http://www-robotics.cs.umass.edu/~grupen/503/SLIDES/cart-pole.pdf},
journal={Online},
year={2018}
}

@article{carracing_v0,
  author = {Oleg Klimov},
  title = {CarRacing-v0},
  year = 2016,
  url = {https://gym.openai.com/envs/CarRacing-v0/},
}

@article{vae,
  title={Auto-Encoding Variational Bayes},
 author = {Kingma, D. and Welling, M.},
journal={Preprint arXiv:1312.6114},
  year={2013},
  url={https://arxiv.org/abs/1312.6114},
}
@article{vae_dm,
  title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
 author = {Rezende, D. and Mohamed, S. and Wierstra, D.},
journal={Preprint arXiv:1401.4082},
  year={2014},
  url={https://arxiv.org/abs/1401.4082},
}

@inproceedings{talvitie2014model,
  title={Model Regularization for Stable Sample Rollouts},
  author={Talvitie, Erik},
  booktitle={UAI},
  pages={780--789},
  year={2014},
  url={http://auai.org/uai2014/proceedings/individuals/179.pdf},
}

@article{asadi2018lipschitz,
  title={Lipschitz continuity in model-based reinforcement learning},
  author={Asadi, Kavosh and Misra, Dipendra and Littman, Michael L},
  journal={arXiv preprint arXiv:1804.07193},
  year={2018},
  url={https://arxiv.org/abs/1804.07193},
}


@article{le2011building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y},
  journal={arXiv preprint arXiv:1112.6209},
  year={2011},
  url={https://arxiv.org/abs/1112.6209},
}


@inproceedings{noroozi2016unsupervised,
  title={Unsupervised learning of visual representations by solving jigsaw puzzles},
  author={Noroozi, Mehdi and Favaro, Paolo},
  booktitle={European Conference on Computer Vision},
  pages={69--84},
  year={2016},
  organization={Springer},
  url={https://arxiv.org/abs/1603.09246},
} 

@inproceedings{sermanet2018time,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1134--1141},
  year={2018},
  organization={IEEE},
  url={https://arxiv.org/abs/1704.06888},
}

@article{higgins2018towards,
  title={Towards a Definition of Disentangled Representations},
  author={Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1812.02230},
  year={2018},
  url={https://arxiv.org/abs/1812.02230},
} 

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018},
  url={https://arxiv.org/abs/1812.00568},
} 

@article{buesing2018learning,
  title={Learning and querying fast generative models for reinforcement learning},
  author={Buesing, Lars and Weber, Theophane and Racaniere, Sebastien and Eslami, SM and Rezende, Danilo and Reichert, David P and Viola, Fabio and Besse, Frederic and Gregor, Karol and Hassabis, Demis and others},
  journal={arXiv preprint arXiv:1802.03006},
  year={2018},
  url={https://arxiv.org/abs/1802.03006},
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE},
  url={https://arxiv.org/abs/1206.5538},
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996},
  url={https://arxiv.org/abs/cs/9605103},
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press},
  url={http://incompleteideas.net/book/the-book-2nd.html},
}

@article{barto1983neuronlike,
  title={Neuronlike adaptive elements that can solve difficult learning control problems},
  author={Barto, Andrew G and Sutton, Richard S and Anderson, Charles W},
  journal={IEEE transactions on systems, man, and cybernetics},
  pages={834--846},
  year={1983},
  publisher={IEEE},
  url={https://bit.ly/31OQlAd},
}

@book{sutton1998introduction,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={135},
  year={1998},
  publisher={MIT press Cambridge},
  url={https://dl.acm.org/citation.cfm?id=551283},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015},
  url={https://arxiv.org/abs/1502.05477},
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE},
  url={https://arxiv.org/abs/1708.02596},
}

@inproceedings{nagabandi2018learning,
  title={Learning Image-Conditioned Dynamics Models for Control of Underactuated Legged Millirobots},
  author={Nagabandi, Anusha and Yang, Guangzhao and Asmar, Thomas and Pandya, Ravi and Kahn, Gregory and Levine, Sergey and Fearing, Ronald S},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={4606--4613},
  year={2018},
  organization={IEEE},
  url={https://people.eecs.berkeley.edu/~ronf/PAPERS/anagabandi-iros18.pdf},
}

@inproceedings{lenz2015deepmpc,
  title={DeepMPC: Learning deep latent features for model predictive control},
  author={Lenz, Ian and Knepper, Ross A and Saxena, Ashutosh},
  booktitle={Robotics: Science and Systems},
  year={2015},
  organization={Rome, Italy},
  url={https://cs.stanford.edu/people/asaxena/papers/deepmpc_rss2015.pdf},
}

@inproceedings{oh2015action,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  booktitle={Advances in neural information processing systems},
  pages={2863--2871},
  year={2015},
  url={https://arxiv.org/abs/1507.08750},
}


@inproceedings{kalchbrenner2017video,
  title={Video pixel networks},
  author={Kalchbrenner, Nal and van den Oord, Aaron and Simonyan, Karen and Danihelka, Ivo and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1771--1779},
  year={2017},
  organization={JMLR.org},
  url={https://arxiv.org/abs/1610.00527},
}

@article{patraucean2015spatio,
  title={Spatio-temporal video autoencoder with differentiable memory},
  author={Patraucean, Viorica and Handa, Ankur and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.06309},
  year={2015},
  url={https://arxiv.org/abs/1511.06309},
}

@article{srivastava2015unsupervised,
  title={Unsupervised learning of video representations using lstms},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1502.04681},
  year={2015},
  url={https://arxiv.org/abs/1502.04681},
}

@article{bull1999model,
  title={On model-based evolutionary computation},
  author={Bull, Larry},
  journal={Soft Computing},
  volume={3},
  number={2},
  pages={76--82},
  year={1999},
  publisher={Springer}
}

@article{mathieu2015deep,
  title={Deep multi-scale video prediction beyond mean square error},
  author={Mathieu, Michael and Couprie, Camille and LeCun, Yann},
  journal={arXiv preprint arXiv:1511.05440},
  year={2015},
  url={https://arxiv.org/abs/1511.05440},
}

@inproceedings{wierstra2008natural,
  title={Natural evolution strategies},
  author={Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  booktitle={2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
  pages={3381--3387},
  year={2008},
  organization={IEEE},
  url={http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf},
}

@book{holland1975adaptation,
  title={Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence},
  author={Holland, John Henry and others},
  year={1975},
  publisher={MIT press},
  url={https://dl.acm.org/citation.cfm?id=531075},
}

@article{goldberg1988genetic,
  title={Genetic algorithms and machine learning},
  author={Goldberg, David E and Holland, John H},
  journal={Machine learning},
  volume={3},
  number={2},
  pages={95--99},
  year={1988},
  publisher={Springer},
  url={https://dl.acm.org/citation.cfm?id=637947},
} 


@article{rechenberg1973evolutionsstrategie,
  title={Evolutionsstrategie--Optimierung technisher Systeme nach Prinzipien der biologischen Evolution},
  journal={Frommann-Holzboog},
  author={Rechenberg, Ingo},
  year={1973},
  publisher={Frommann-Holzboog}
}

@article{hansen2003reducing,
  title={Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)},
  author={Hansen, Nikolaus and Meuller, Sibylle D and Koumoutsakos, Petros},
  journal={Evolutionary computation},
  volume={11},
  number={1},
  pages={1--18},
  year={2003},
  publisher={MIT Press},
  url={http://www.cmap.polytechnique.fr/~nikolaus.hansen/evco_11_1_1_0.pdf},
}

@article{bongard2006resilient,
  title={Resilient machines through continuous self-modeling},
  author={Bongard, Josh and Zykov, Victor and Lipson, Hod},
  journal={Science},
  volume={314},
  number={5802},
  pages={1118--1121},
  year={2006},
  publisher={American Association for the Advancement of Science},
  url={https://bit.ly/2P2678d},
}

@article{lehman2018surprising,
  title={The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities},
  author={Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J and Bernard, Samuel and Beslon, Guillaume and Bryson, David M and others},
  journal={arXiv preprint arXiv:1803.03453},
  year={2018},
  url={https://arxiv.org/abs/1803.03453},
}

@book{schwefel1977numerische,
  title={Numerische Optimierung von Computer-Modellen mittels der Evolutionsstrategie.(Teil 1, Kap. 1-5)},
  author={Schwefel, H-P},
  year={1977},
  publisher={Birkhauser}
}

@inproceedings{amos2018differentiable,
  title={Differentiable MPC for End-to-end Planning and Control},
  author={Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8289--8300},
  year={2018},
  url={https://arxiv.org/abs/1810.13400},
}

@article{schmidhuber2015learning,
  title={On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models},
  author={Schmidhuber, J.},
  journal={arXiv preprint arXiv:1511.09249},
  year={2015},
  url={https://arxiv.org/abs/1511.09249},
}
</script>
<!--


-->
<script language="javascript" type="text/javascript" src="lib/p5.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/p5.dom.js"></script>
<script language="javascript" type="text/javascript" src="lib/numjs.js"></script>
<script src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/controller.js"></script>
